{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e0bf35-34e5-4bb8-bf39-1c3fe036bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS598 Deep Learning for Healthcare\n",
    "# Calibrated Random Forest - Results Reproduction\n",
    "# Grant Espe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f267efe-0b82-4605-853e-81bb07df91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.isotonic import IsotonicRegression as Iso\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db5175aa-9e19-4b21-ad78-a4e3e621169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CaliForest Class - https://github.com/yubin-park/califorest/blob/master/califorest/califorest.py\n",
    "\n",
    "class CaliForest(ClassifierMixin, BaseEstimator):\n",
    "    def __init__(self,\n",
    "                n_estimators=300,\n",
    "                criterion=\"gini\",\n",
    "                max_depth=5,\n",
    "                min_samples_split=2,\n",
    "                min_samples_leaf=1,\n",
    "                ctype=\"isotonic\",\n",
    "                alpha0=100,\n",
    "                beta0=25):\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.ctype = ctype\n",
    "        self.alpha0 = alpha0\n",
    "        self.beta0 = beta0\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, accept_sparse=False)\n",
    "        self.estimators = []\n",
    "        self.calibrator = None\n",
    "        for i in range(self.n_estimators):\n",
    "            self.estimators.append(Tree(criterion=self.criterion,\n",
    "                                        max_depth=self.max_depth,\n",
    "                                        min_samples_split=self.min_samples_split,\n",
    "                                        min_samples_leaf=self.min_samples_leaf,\n",
    "                                      #  max_features=\"auto\")) failing on newer version of DecisionTreeClassifier\n",
    "                                        max_features=\"sqrt\"))\n",
    "\n",
    "        if self.ctype==\"logistic\":\n",
    "            #self.calibrator = LR(penalty=\"none\", #Changed from 'none' -> None\n",
    "            self.calibrator = LR(penalty=None,\n",
    "                                 solver=\"saga\",\n",
    "                                 max_iter=5000)\n",
    "        elif self.ctype==\"isotonic\":\n",
    "            self.calibrator = Iso(y_min=0,\n",
    "                                y_max=1,\n",
    "                                out_of_bounds=\"clip\")\n",
    "        n, m  = X.shape\n",
    "        Y_oob = np.full((n, self.n_estimators), np.nan)\n",
    "        n_oob = np.zeros(n)\n",
    "        IB = np.zeros((n, self.n_estimators), dtype=int)\n",
    "        OOB = np.full((n, self.n_estimators), True)\n",
    "        \n",
    "        for eid in range(self.n_estimators):\n",
    "            IB[:,eid] = np.random.choice(n, n)\n",
    "            OOB[IB[:,eid],eid] = False\n",
    "\n",
    "        for eid, est in enumerate(self.estimators):\n",
    "            ib_idx = IB[:,eid]\n",
    "            oob_idx = OOB[:,eid]\n",
    "            est.fit(X[ib_idx,:], y[ib_idx])\n",
    "            Y_oob[oob_idx,eid] = est.predict_proba(X[oob_idx,:])[:,1]\n",
    "            n_oob[oob_idx] += 1\n",
    "\n",
    "\n",
    "        oob_idx = n_oob > 1\n",
    "        Y_oob_ = Y_oob[oob_idx,:]\n",
    "        n_oob_ = n_oob[oob_idx]\n",
    "        z_hat = np.nanmean(Y_oob_, axis=1)\n",
    "        z_true = y[oob_idx]\n",
    "\n",
    "        beta = self.beta0 + np.nanvar(Y_oob_, axis=1) * n_oob_ / 2\n",
    "        alpha = self.alpha0 + n_oob_/2\n",
    "        z_weight = alpha / beta\n",
    "\n",
    "        if self.ctype==\"logistic\":\n",
    "            self.calibrator.fit(z_hat[:,np.newaxis], z_true, z_weight)\n",
    "        elif self.ctype==\"isotonic\":\n",
    "            self.calibrator.fit(z_hat, z_true, z_weight)\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = check_array(X)\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        \n",
    "        n, m = X.shape\n",
    "        n_est = len(self.estimators)\n",
    "        z = np.zeros(n)\n",
    "        y_mat = np.zeros((n,2))\n",
    "        for eid, est in enumerate(self.estimators):\n",
    "            z += est.predict_proba(X)[:,1]\n",
    "        z /= n_est\n",
    "\n",
    "        if self.ctype==\"logistic\":\n",
    "            y_mat[:,1] = self.calibrator.predict_proba(z[:,np.newaxis])[:,1]\n",
    "        elif self.ctype==\"isotonic\":\n",
    "            y_mat[:,1] = self.calibrator.predict(z)\n",
    "            \n",
    "        y_mat[:,0] = 1 - y_mat[:,1]\n",
    "        return y_mat\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.argmax(proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7382f324-2780-459b-9e6f-d6fab7b0d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RC30 Class - https://github.com/yubin-park/califorest/blob/master/califorest/rc30.py\n",
    "\n",
    "class RC30(ClassifierMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, \n",
    "                n_estimators=30, \n",
    "                max_depth=3,\n",
    "                min_samples_split=2,\n",
    "                min_samples_leaf=1, \n",
    "                ctype=\"isotonic\"):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.ctype = ctype\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.model = RandomForestClassifier(n_estimators=self.n_estimators,\n",
    "                                            max_depth=self.max_depth,\n",
    "                                            min_samples_split=self.min_samples_split,\n",
    "                                            min_samples_leaf=self.min_samples_leaf)\n",
    "        if self.ctype == \"logistic\":\n",
    "            self.calibrator = LR(C=1e20, solver=\"lbfgs\")\n",
    "        elif self.ctype == \"isotonic\":\n",
    "            self.calibrator = Iso(y_min=0, y_max=1,\n",
    "                                                out_of_bounds=\"clip\")\n",
    "        X0, X1, y0, y1 = train_test_split(X, y, test_size=0.3) \n",
    "        self.model.fit(X0, y0)\n",
    "        if self.ctype == \"logistic\":\n",
    "            y_est = self.model.predict_proba(X1)[:,[1]]\n",
    "            self.calibrator.fit(y_est, y1)\n",
    "        elif self.ctype == \"isotonic\":\n",
    "            y_est = self.model.predict_proba(X1)[:,1]\n",
    "            self.calibrator.fit(y_est, y1)\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    " \n",
    "    def predict_proba(self, X):\n",
    "        X = check_array(X)\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        \n",
    "        if self.ctype == \"logistic\":\n",
    "            return self.calibrator.predict_proba(\n",
    "                    self.model.predict_proba(X)[:,[1]])\n",
    "        elif self.ctype == \"isotonic\":\n",
    "            n, m = X.shape\n",
    "            y = np.zeros((n,2))\n",
    "            y[:,1] = self.calibrator.predict(\n",
    "                        self.model.predict_proba(X)[:,1])\n",
    "            y[:,0] = 1 - y[:,1]\n",
    "            return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73ad1aa9-85af-46de-b24b-f15685aa674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CaliForest Evaluation Metrics - https://github.com/yubin-park/califorest/blob/master/califorest/metrics/eval_metrics.py\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import norm\n",
    "\n",
    "def hosmer_lemeshow(y_true, y_score):\n",
    "    \"\"\"\n",
    "    Calculate the Hosmer Lemeshow to assess whether\n",
    "    or not the observed event rates match expected\n",
    "    event rates.\n",
    "\n",
    "    Assume that there are 10 groups:\n",
    "    HL = \\\\sum_{g=1}^G \\\\frac{(O_{1g} - E_{1g})^2}{N_g \\\\pi_g (1- \\\\pi_g)}\n",
    "    \"\"\"\n",
    "    n_grp = 10 # number of groups\n",
    "\n",
    "    # create the dataframe\n",
    "    df = pd.DataFrame({'score': y_score, 'target': y_true})\n",
    "\n",
    "    # sort the values\n",
    "    df = df.sort_values('score')\n",
    "    # shift the score a bit\n",
    "    df['score'] = np.clip(df['score'], 1e-8, 1-1e-8)\n",
    "    df['rank'] = list(range(df.shape[0]))\n",
    "    # cut them into 10 bins\n",
    "    df['score_decile'] = pd.qcut(df['rank'], n_grp,\n",
    "                                      duplicates='raise')\n",
    "    # sum up based on each decile\n",
    "    obsPos = df['target'].groupby(df.score_decile).sum()\n",
    "    obsNeg = (df['target'].groupby(df.score_decile).count() - \n",
    "                obsPos)\n",
    "    exPos = df['score'].groupby(df.score_decile).sum()\n",
    "    exNeg = df['score'].groupby(df.score_decile).count() - exPos\n",
    "    hl = (((obsPos - exPos)**2/exPos) + ((obsNeg - exNeg)**2/exNeg)).sum()\n",
    "\n",
    "    # https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n",
    "    # Re: p-value, higher the better Goodness-of-Fit\n",
    "    p_value = 1 - chi2.cdf(hl, n_grp-2)\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "def reliability(y_true, y_score):\n",
    "\n",
    "    n_grp = 10\n",
    "    df = pd.DataFrame({'score': y_score, 'target': y_true})\n",
    "    df = df.sort_values('score')\n",
    "    df['rank'] = list(range(df.shape[0]))\n",
    "    df['score_decile'] = pd.qcut(df['rank'], n_grp,\n",
    "                                      duplicates='raise')\n",
    "\n",
    "    obs = df['target'].groupby(df.score_decile).mean()\n",
    "    exp = df['score'].groupby(df.score_decile).mean()\n",
    "\n",
    "    rel_small = np.mean((obs - exp)**2)\n",
    "    rel_large = (np.mean(y_true) - np.mean(y_score))**2\n",
    "\n",
    "    return rel_small, rel_large\n",
    "\n",
    "def spiegelhalter(y_true, y_score):\n",
    "    top = np.sum((y_true - y_score)*(1-2*y_score))\n",
    "    bot = np.sum((1-2*y_score)**2 * y_score * (1-y_score))\n",
    "    sh = top / np.sqrt(bot)\n",
    "\n",
    "    # https://en.wikipedia.org/wiki/Z-test\n",
    "    # Two-tailed test\n",
    "    # Re: p-value, higher the better Goodness-of-Fit\n",
    "    p_value = norm.sf(np.abs(sh)) * 2\n",
    "\n",
    "    return p_value\n",
    "\n",
    "def scaled_brier_score(y_true, y_score):\n",
    "    brier = skm.brier_score_loss(y_true, y_score)\n",
    "    # calculate the mean of the probability\n",
    "    p = np.mean(y_true)  \n",
    "    brier_scaled = 1 - brier / (p * (1-p))\n",
    "    return brier, brier_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cbab935-1f2b-4644-93b6-a24b808ddc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Loading MIMIC-Extract Dataset - https://github.com/yubin-park/califorest/blob/master/analysis/mimic_extract.py\n",
    "\n",
    "GAP_TIME          = 6  # In hours\n",
    "WINDOW_SIZE       = 24 # In hours\n",
    "ID_COLS           = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "TRAIN_FRAC, TEST_FRAC = 0.7, 0.3\n",
    "\n",
    "def simple_imputer(df):\n",
    "    idx = pd.IndexSlice\n",
    "    df = df.copy()\n",
    "    if len(df.columns.names) > 2: \n",
    "        df.columns = df.columns.droplevel(('label', 'LEVEL1', 'LEVEL2'))\n",
    "    \n",
    "    df_out = df.loc[:, idx[:, ['mean', 'count']]]\n",
    "    \n",
    "    icustay_means = df_out.loc[:, idx[:, 'mean']].groupby(ID_COLS).mean()\n",
    "    imputed_means = (df_out.loc[:,idx[:,'mean']]\n",
    "                           .groupby(ID_COLS).fillna(method='ffill')\n",
    "                           .groupby(ID_COLS).fillna(icustay_means)\n",
    "                           .fillna(0)).copy()\n",
    "    df_out.loc[:,idx[:,'mean']] = imputed_means\n",
    "\n",
    "    mask = (df.loc[:, idx[:,'count']] > 0).astype(float).copy()\n",
    "    df_out.loc[:,idx[:,'count']] = mask\n",
    "    df_out = df_out.rename(columns={'count': 'mask'}, \n",
    "                            level='Aggregation Function')\n",
    "   \n",
    "    is_absent = (1 - df_out.loc[:, idx[:,'mask']].copy())\n",
    "    hours_of_absence = is_absent.cumsum()\n",
    "    time_since_measured = (hours_of_absence \n",
    "                           - (hours_of_absence[is_absent==0]\n",
    "                                .fillna(method='ffill')))\n",
    "    time_since_measured.rename(columns={'mask': 'time_since_measured'}, \n",
    "                               level='Aggregation Function', \n",
    "                               inplace=True)\n",
    "\n",
    "    df_out = pd.concat((df_out, time_since_measured), axis=1)\n",
    "    time_since_measured = (df_out.loc[:, idx[:, 'time_since_measured']]\n",
    "                                .fillna(WINDOW_SIZE+1)).copy()\n",
    "    df_out.loc[:, idx[:, 'time_since_measured']] = time_since_measured\n",
    "    df_out.sort_index(axis=1, inplace=True)\n",
    "    return df_out\n",
    "\n",
    "def mimic_extract(random_seed, target):\n",
    "\n",
    "    statics = pd.read_hdf(\"data/all_hourly_data.h5\", 'patients')\n",
    "    data_full_lvl2 = pd.read_hdf(\"data/all_hourly_data.h5\", 'vitals_labs')\n",
    "\n",
    "    statics = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME]\n",
    "    Ys = statics.loc[:,['mort_hosp', 'mort_icu', 'los_icu']]\n",
    "    Ys.loc[:,\"mort_hosp\"] = (Ys.loc[:,\"mort_hosp\"]).astype(int)\n",
    "    Ys.loc[:,\"mort_icu\"] = (Ys.loc[:,\"mort_icu\"]).astype(int)\n",
    "    Ys.loc[:,'los_3'] = (Ys.loc[:,'los_icu'] > 3).astype(int)\n",
    "    Ys.loc[:,'los_7'] = (Ys.loc[:,'los_icu'] > 7).astype(int)\n",
    "    Ys.drop(columns=['los_icu'], inplace=True)\n",
    "\n",
    "    lvl2 = data_full_lvl2.loc[(data_full_lvl2\n",
    "                    .index.get_level_values('icustay_id')\n",
    "                    .isin(set(Ys.index.get_level_values('icustay_id')))) &\n",
    "                (data_full_lvl2\n",
    "                    .index.get_level_values('hours_in') < WINDOW_SIZE),:] \n",
    "\n",
    "    lvl2_subj_idx, Ys_subj_idx = [df.index.get_level_values('subject_id') \n",
    "                                    for df in (lvl2, Ys)]\n",
    "    lvl2_subjects = set(lvl2_subj_idx)\n",
    "    assert lvl2_subjects == set(Ys_subj_idx), \"Subject ID pools differ!\"\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    subjects = np.random.permutation(list(lvl2_subjects))\n",
    "    N = len(lvl2_subjects)\n",
    "    N_train, N_test = int(TRAIN_FRAC * N), int(TEST_FRAC * N)\n",
    "    train_subj = subjects[:N_train]\n",
    "    test_subj  = subjects[N_train:]\n",
    "\n",
    "    [(lvl2_train, lvl2_test), (Ys_train, Ys_test)] = [\n",
    "        [df.loc[df.index.get_level_values('subject_id').isin(s),:] \n",
    "            for s in (train_subj, test_subj)] for df in (lvl2, Ys)]\n",
    "\n",
    "    idx = pd.IndexSlice\n",
    "    lvl2_means = lvl2_train.loc[:,idx[:,'mean']].mean(axis=0)\n",
    "\n",
    "    # here, we do not use the standard deviation. \n",
    "    # Tree models do not get affected by the scale of values.\n",
    "    lvl2_stds = lvl2_train.loc[:,idx[:,'mean']].std(axis=0)\n",
    "\n",
    "    vals_centered = (lvl2_train.loc[:,idx[:,'mean']] - lvl2_means)\n",
    "    lvl2_train.loc[:,idx[:,'mean']] = vals_centered\n",
    "    vals_centered = (lvl2_test.loc[:,idx[:,'mean']] - lvl2_means)\n",
    "    lvl2_test.loc[:,idx[:,'mean']] = vals_centered\n",
    "\n",
    "    lvl2_train, lvl2_test = [simple_imputer(df) \n",
    "                                for df in (lvl2_train, lvl2_test)]\n",
    "\n",
    "    lvl2_flat_train, lvl2_flat_test = [(df\n",
    "                .pivot_table(index=['subject_id', 'hadm_id', 'icustay_id'], \n",
    "                                               columns=['hours_in'])) \n",
    "                                   for df in (lvl2_train, lvl2_test)]\n",
    "    \n",
    "\n",
    "    return (lvl2_flat_train.values, \n",
    "            lvl2_flat_test.values,\n",
    "            Ys_train.loc[:,target].values,\n",
    "            Ys_test.loc[:,target].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e84f9-551f-4011-a69d-49348a9e94b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] mimic3_mort_hosp CF-Iso: 310.774 sec & BS 0.07251\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pprint import pprint\n",
    "#import argparse\n",
    "import time\n",
    "from itertools import product\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#from califorest import CaliForest\n",
    "#from califorest import RC30\n",
    "#from califorest import metrics as em\n",
    "#import mimic_extract as mimic\n",
    "\n",
    "def read_data(dataset, random_seed):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None\n",
    "    \n",
    "    if dataset == \"hastie\":\n",
    "        np.random.seed(random_seed)\n",
    "        poly = PolynomialFeatures()\n",
    "        X, y = make_hastie_10_2(n_samples=10000)\n",
    "        X = poly.fit_transform(X)\n",
    "        y[y<0] = 0\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3)\n",
    "    elif dataset == \"breast_cancer\":\n",
    "        np.random.seed(random_seed)\n",
    "        poly = PolynomialFeatures()\n",
    "        X, y = load_breast_cancer(return_X_y=True)\n",
    "        X = poly.fit_transform(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3)\n",
    "    elif dataset == \"mimic3_mort_hosp\":\n",
    "        X_train, X_test, y_train, y_test = mimic_extract(random_seed, \n",
    "                                                        \"mort_hosp\")\n",
    "    elif dataset == \"mimic3_mort_icu\":\n",
    "        X_train, X_test, y_train, y_test = mimic_extract(random_seed, \n",
    "                                                        \"mort_icu\")\n",
    "    elif dataset == \"mimic3_los_3\":\n",
    "        X_train, X_test, y_train, y_test = mimic_extract(random_seed, \n",
    "                                                        \"los_3\")\n",
    "    elif dataset == \"mimic3_los_7\":\n",
    "        X_train, X_test, y_train, y_test = mimic_extract(random_seed, \n",
    "                                                        \"los_7\")\n",
    " \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def init_models(n_estimators, max_depth):\n",
    "\n",
    "    mss = 3\n",
    "    msl = 1\n",
    "    models = {\"CF-Iso\": CaliForest(n_estimators=n_estimators,\n",
    "                                        max_depth=max_depth,\n",
    "                                        min_samples_split=mss,\n",
    "                                        min_samples_leaf=msl,\n",
    "                                        ctype=\"isotonic\"),\n",
    "            \"CF-Logit\": CaliForest(n_estimators=n_estimators,\n",
    "                                        max_depth=max_depth,\n",
    "                                        min_samples_split=mss,\n",
    "                                        min_samples_leaf=msl,\n",
    "                                        ctype=\"logistic\"),\n",
    "            \"RC-Iso\": RC30(n_estimators=n_estimators,\n",
    "                                        max_depth=max_depth,\n",
    "                                        min_samples_split=mss,\n",
    "                                        min_samples_leaf=msl,\n",
    "                                        ctype=\"isotonic\"),\n",
    "            \"RC-Logit\": RC30(n_estimators=n_estimators,\n",
    "                                        max_depth=max_depth,\n",
    "                                        min_samples_split=mss,\n",
    "                                        min_samples_leaf=msl,\n",
    "                                        ctype=\"logistic\"),\n",
    "            \"RF-NoCal\": RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                        max_depth=max_depth,\n",
    "                                        min_samples_split=mss,\n",
    "                                        min_samples_leaf=msl)}\n",
    "    return models\n",
    "\n",
    "\n",
    "def run(dataset, random_seed, n_estimators=300, depth=10):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = read_data(dataset, random_seed)\n",
    "\n",
    "    output = []\n",
    "\n",
    "    models = init_models(n_estimators, depth)\n",
    " \n",
    "    for name, model in models.items():\n",
    "        t_start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        t_elapsed = time.time() - t_start\n",
    "        y_pred = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "        score_auc = roc_auc_score(y_test, y_pred)\n",
    "       # score_hl = em.hosmer_lemeshow(y_test, y_pred)\n",
    "       # score_sh = em.spiegelhalter(y_test, y_pred)\n",
    "       # score_b, score_bs = em.scaled_brier_score(y_test, y_pred)\n",
    "       # rel_small, rel_large = em.reliability(y_test, y_pred)\n",
    "\n",
    "        # Since I load the functions manually in earlier cell:\n",
    "        score_hl = hosmer_lemeshow(y_test, y_pred)\n",
    "        score_sh = spiegelhalter(y_test, y_pred)\n",
    "        score_b, score_bs = scaled_brier_score(y_test, y_pred)\n",
    "        rel_small, rel_large = reliability(y_test, y_pred)\n",
    "\n",
    "        row = [dataset, name, random_seed, \n",
    "               score_auc, score_b, score_bs, score_hl, score_sh,\n",
    "               rel_small, rel_large] \n",
    "\n",
    "        print((\"[info] {} {}: {:.3f} sec & BS {:.5f}\").format(\n",
    "                dataset, name, t_elapsed, score_b))\n",
    "\n",
    "        output.append(row)\n",
    "\n",
    "    return output\n",
    "       \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "   # parser = argparse.ArgumentParser()\n",
    "  #  parser.add_argument(\"dataset\", type=str)\n",
    "  #  args = parser.parse_args()\n",
    "\n",
    "    output = [[\"dataset\", \"model\",\n",
    "                \"random_seed\", \"auc\", \"brier\", \"brier_scaled\", \n",
    "                \"hosmer_lemshow\", \"speigelhalter\",\n",
    "                \"reliability_small\", \"reliability_large\"]]\n",
    "   \n",
    "    # Choose from one of the 6 datasets\n",
    "    #dataset = \"hastie\"\n",
    "    #dataset = \"breast_cancer\"\n",
    "    \n",
    "    #dataset = \"mimic3_los_3\"\n",
    "    #dataset = \"mimic3_los_7\"\n",
    "    #dataset = \"mimic3_mort_icu\"\n",
    "    dataset = \"mimic3_mort_hosp\"\n",
    "\n",
    "    # Adjust the number of estimators and depth of trees according to the paper\n",
    "    n = 300\n",
    "    d = 10\n",
    "\n",
    "    for rs in range(10):\n",
    "        #output += run(args.dataset, rs)\n",
    "        output += run(dataset, rs, n_estimators=n, depth=d)\n",
    "\n",
    "    fn = \"results/{}.csv\".format(dataset)\n",
    "    with open(fn, \"w\") as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerows(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b46a3-a869-467f-b021-b1eebc9f4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Plots - Converted from their R Code into Python\n",
    "# https://github.com/yubin-park/califorest/blob/master/analysis/plot_chil_exp.R\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "palette = [\"#2b83ba\", \"#abdda4\", \"#d7191c\", \"#fdae61\", \"#b0b0b0\"]\n",
    "fn = os.path.join(\"results\", f\"{dataset}.csv\")\n",
    "df = pd.read_csv(fn)\n",
    "\n",
    "#long format dataframe like rbind in R\n",
    "df_brier_scaled = pd.DataFrame({'model': df['model'], 'var': 'Scaled Brier Score', 'val': df['brier_scaled']})\n",
    "df_hosmer = pd.DataFrame({'model': df['model'], 'var': 'Hosmer-Lemeshow p-value', 'val': df['hosmer_lemshow']})\n",
    "df_spiegelhalter = pd.DataFrame({'model': df['model'], 'var': 'Spiegelhalter p-value', 'val': df['speigelhalter']})\n",
    "df_brier = pd.DataFrame({'model': df['model'], 'var': 'Brier Score', 'val': df['brier']})\n",
    "df_reliability_small = pd.DataFrame({'model': df['model'], 'var': 'Reliability-in-the-small', 'val': df['reliability_small']})\n",
    "df_reliability_large = pd.DataFrame({'model': df['model'], 'var': 'Reliability-in-the-large', 'val': df['reliability_large']})\n",
    "\n",
    "#merge dataframes\n",
    "df_long = pd.concat([df_brier_scaled, df_hosmer, df_spiegelhalter, df_brier, \n",
    "                     df_reliability_small, df_reliability_large], ignore_index=True)\n",
    "\n",
    "\n",
    "### orignal authors remove the CF-Logit model for these two graphs, due to inconsistancy\n",
    "if dataset in [\"mimic3_los_7\", \"mimic3_los_3\"]:\n",
    "    df_long = df_long[df_long['model'] != 'CF-Logit']\n",
    "    palette = palette[:4]\n",
    "\n",
    "#plot\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "g = sns.FacetGrid(df_long, col='var', col_wrap=3, sharey=False, height=2.5)\n",
    "\n",
    "#add boxplots\n",
    "g.map_dataframe(sns.boxplot, x='model', y='val', hue='model', palette=palette)\n",
    "\n",
    "#rotate labels, formatting\n",
    "for ax in g.axes.flat:\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha='right')\n",
    "    ax.set_ylabel('')\n",
    "    ax.get_legend().remove() if ax.get_legend() else None\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"results\", f\"{dataset}_plots.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f1ff0-2890-46ad-8605-f6e7725d0bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
